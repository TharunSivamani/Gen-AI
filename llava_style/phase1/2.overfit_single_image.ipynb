{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Load Vision Encoder (Frozen)\n",
    "# ----------------------------------\n",
    "\n",
    "vision_name = \"google/siglip-base-patch16-224\"\n",
    "processor = AutoProcessor.from_pretrained(vision_name, use_fast=True)\n",
    "vision_model = AutoModel.from_pretrained(vision_name).vision_model.to(device)\n",
    "vision_model.eval()\n",
    "\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Hidden State:  1024\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# Load LLM\n",
    "# ----------------------------------\n",
    "\n",
    "llm_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    ").to(device)\n",
    "\n",
    "llm_hidden = llm.config.hidden_size\n",
    "\n",
    "print(\"LLM Hidden State: \", llm_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Projection Layer (Trainable)\n",
    "# ----------------------------------\n",
    "\n",
    "projector = nn.Linear(768, llm_hidden, bias=False).to(device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Single Image + Caption\n",
    "# ----------------------------------\n",
    "\n",
    "image = Image.open(\"dogs-playing-in-grassy-field.jpg\").convert(\"RGB\")\n",
    "caption = \"Two dogs playing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Tokens Size:  torch.Size([1, 196, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    vision_outputs = vision_model(**inputs)\n",
    "    patch_tokens_raw = vision_outputs.last_hidden_state # (1, 196, 768)\n",
    "\n",
    "print(\"Patch Tokens Size: \", patch_tokens_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Text Prompt\n",
    "# ----------------------------------\n",
    "\n",
    "prompt = f\"USER: <image>\\nDescribe the image.\\nASSISTANT: {caption}\"\n",
    "\n",
    "tokens = tokenizer(\n",
    "    prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "input_ids = tokens.input_ids\n",
    "attention_mask = tokens.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Prepare Labels (IMPORTANT)\n",
    "# ----------------------------------\n",
    "\n",
    "labels = input_ids.clone()\n",
    "\n",
    "# Mask everything except assistant answer\n",
    "assistant_start = prompt.index(\"ASSISTANT:\")\n",
    "assistant_tokens = tokenizer(\n",
    "    prompt[:assistant_start],\n",
    "    return_tensors=\"pt\"\n",
    ").input_ids.shape[1]\n",
    "\n",
    "labels[:, :assistant_tokens] = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=1024, bias=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# Training Setup\n",
    "# ----------------------------------\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(projector.parameters()) + list(llm.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "llm.train()\n",
    "projector.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Loss: 14.728104591369629\n",
      "Step 50 | Loss: 3.994830331066623e-05\n",
      "Step 100 | Loss: 1.877509566838853e-05\n",
      "Step 150 | Loss: 1.3455543012241833e-05\n",
      "Step 200 | Loss: 1.0385981113358866e-05\n",
      "Step 250 | Loss: 8.329663614858873e-06\n",
      "Step 300 | Loss: 6.8693752837134525e-06\n",
      "Step 350 | Loss: 5.7816068874672055e-06\n",
      "Step 400 | Loss: 4.962053935742006e-06\n",
      "Step 450 | Loss: 4.261707545083482e-06\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------\n",
    "# Training Loop\n",
    "# ----------------------------------\n",
    "\n",
    "for step in range(500):\n",
    "    \n",
    "    patch_tokens = patch_tokens_raw.to(torch.bfloat16)\n",
    "    patch_tokens = projector(patch_tokens)\n",
    "\n",
    "    text_embeds = llm.get_input_embeddings()(input_ids)\n",
    "\n",
    "    inputs_embeds = torch.cat([patch_tokens, text_embeds], dim=1)\n",
    "\n",
    "    visual_attention = torch.ones(\n",
    "        (1, patch_tokens.size(1)),\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    full_attention = torch.cat(\n",
    "        [visual_attention, attention_mask],\n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "    # Pad labels for visual tokens\n",
    "    visual_label_pad = torch.full(\n",
    "        (1, patch_tokens.size(1)),\n",
    "        -100,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    full_labels = torch.cat(\n",
    "        [visual_label_pad, labels],\n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "    outputs = llm(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=full_attention,\n",
    "        labels=full_labels\n",
    "    )\n",
    "\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs\n"
     ]
    }
   ],
   "source": [
    "llm.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated = llm.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=full_attention,\n",
    "        max_new_tokens=50\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct generation test\n",
    "\n",
    "inference_prompt = \"USER: <image>\\nDescribe the image.\\nASSISTANT:\"\n",
    "\n",
    "tokens = tokenizer(\n",
    "    inference_prompt,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "input_ids = tokens.input_ids\n",
    "attention_mask = tokens.attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeds = llm.get_input_embeddings()(input_ids)\n",
    "\n",
    "patch_tokens = patch_tokens_raw.to(torch.bfloat16)\n",
    "patch_tokens = projector(patch_tokens)\n",
    "\n",
    "inputs_embeds = torch.cat([patch_tokens, text_embeds], dim=1)\n",
    "\n",
    "visual_attention = torch.ones(\n",
    "    (1, patch_tokens.size(1)),\n",
    "    device=device\n",
    ")\n",
    "\n",
    "full_attention = torch.cat(\n",
    "    [visual_attention, attention_mask],\n",
    "    dim=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs\n"
     ]
    }
   ],
   "source": [
    "generated = llm.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=full_attention,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs playing. Two dogs\n"
     ]
    }
   ],
   "source": [
    "# testing vision ablation\n",
    "\n",
    "zero_visual = torch.zeros_like(patch_tokens)\n",
    "\n",
    "inputs_embeds = torch.cat([zero_visual, text_embeds], dim=1)\n",
    "\n",
    "generated = llm.generate(\n",
    "    inputs_embeds=inputs_embeds,\n",
    "    attention_mask=full_attention,\n",
    "    max_new_tokens=30,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
