{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1Ô∏è‚É£ Load Vision Encoder (Frozen)\n",
    "# =====================================================\n",
    "\n",
    "vision_name = \"google/siglip-base-patch16-224\"\n",
    "processor = AutoProcessor.from_pretrained(vision_name, use_fast=True)\n",
    "\n",
    "vision_model = AutoModel.from_pretrained(\n",
    "    vision_name\n",
    ").vision_model.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "vision_model.eval()\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 2Ô∏è‚É£ Load LLM\n",
    "# =====================================================\n",
    "\n",
    "llm_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "llm_hidden = llm.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3Ô∏è‚É£ Projection Layer\n",
    "# =====================================================\n",
    "\n",
    "projector = nn.Linear(768, llm_hidden, bias=False).to(device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 4Ô∏è‚É£ Load Two Images + Captions\n",
    "# =====================================================\n",
    "\n",
    "image_A = Image.open(\"images/airplane.png\").convert(\"RGB\")\n",
    "caption_A = \"A large passenger airplane flying through the air.\"\n",
    "\n",
    "image_B = Image.open(\"images/motorcycle.png\").convert(\"RGB\")\n",
    "caption_B = \"Riding a motorcycle down a street.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image):\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = vision_model(**inputs)\n",
    "        return outputs.last_hidden_state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_A = encode_image(image_A)\n",
    "patch_B = encode_image(image_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 5Ô∏è‚É£ Tokenize Captions\n",
    "# =====================================================\n",
    "\n",
    "tokens_A = tokenizer(caption_A, return_tensors=\"pt\").to(device)\n",
    "tokens_B = tokenizer(caption_B, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 6Ô∏è‚É£ Optimizer\n",
    "# =====================================================\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(projector.parameters()) + list(llm.parameters()),\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Loss: 10.959513\n",
      "Step 100 | Loss: 0.000117\n",
      "Step 200 | Loss: 0.000099\n",
      "Step 300 | Loss: 0.000085\n",
      "Step 400 | Loss: 0.000078\n",
      "Step 500 | Loss: 0.000072\n",
      "Step 600 | Loss: 0.000067\n",
      "Step 700 | Loss: 0.000065\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 7Ô∏è‚É£ Training Loop\n",
    "# =====================================================\n",
    "\n",
    "llm.train()\n",
    "projector.train()\n",
    "\n",
    "for step in range(800):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for patch_raw, tokens in [(patch_A, tokens_A), (patch_B, tokens_B)]:\n",
    "\n",
    "        patch_tokens = projector(patch_raw)\n",
    "\n",
    "        text_embeds = llm.get_input_embeddings()(tokens.input_ids)\n",
    "\n",
    "        inputs_embeds = torch.cat([patch_tokens, text_embeds], dim=1)\n",
    "\n",
    "        visual_attention = torch.ones(\n",
    "            (1, patch_tokens.size(1)),\n",
    "            device=device,\n",
    "            dtype=tokens.attention_mask.dtype\n",
    "        )\n",
    "\n",
    "        full_attention = torch.cat(\n",
    "            [visual_attention, tokens.attention_mask],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        visual_label_pad = torch.full(\n",
    "            (1, patch_tokens.size(1)),\n",
    "            -100,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        full_labels = torch.cat(\n",
    "            [visual_label_pad, tokens.input_ids],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        outputs = llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=full_attention,\n",
    "            labels=full_labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step} | Loss: {total_loss/2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 8Ô∏è‚É£ Evaluation Function\n",
    "# =====================================================\n",
    "\n",
    "def generate_caption(patch_raw):\n",
    "\n",
    "    llm.eval()\n",
    "    projector.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        patch_tokens = projector(patch_raw)\n",
    "\n",
    "        visual_attention = torch.ones(\n",
    "            (1, patch_tokens.size(1)),\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        generated = llm.generate(\n",
    "            inputs_embeds=patch_tokens,\n",
    "            attention_mask=visual_attention,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST RESULTS ===\n",
      "Image A ‚Üí A large passenger airplane flying through the air.A\n",
      "Image B ‚Üí Riding a motorcycle down a street.Riding\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 9Ô∏è‚É£ Test Both Images\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n=== TEST RESULTS ===\")\n",
    "print(\"Image A ‚Üí\", generate_caption(patch_A))\n",
    "print(\"Image B ‚Üí\", generate_caption(patch_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ZERO VISION TEST ===\n",
      "Zero vision ‚Üí !iding a motorcycle down a street. A street\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# üîü Zero Vision Test\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n=== ZERO VISION TEST ===\")\n",
    "\n",
    "zero_tokens = torch.zeros_like(projector(patch_A))\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_zero = llm.generate(\n",
    "        inputs_embeds=zero_tokens,\n",
    "        attention_mask=torch.ones(\n",
    "            (1, zero_tokens.size(1)),\n",
    "            device=device\n",
    "        ),\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "print(\"Zero vision ‚Üí\", tokenizer.decode(generated_zero[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
