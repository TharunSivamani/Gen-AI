{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 1️⃣ Load Vision Encoder (Frozen, BF16)\n",
    "# =====================================================\n",
    "\n",
    "vision_name = \"google/siglip-base-patch16-224\"\n",
    "processor = AutoProcessor.from_pretrained(vision_name)\n",
    "vision_model = AutoModel.from_pretrained(vision_name).vision_model.to(device, dtype=torch.bfloat16)\n",
    "vision_model.eval()\n",
    "\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 2️⃣ Load LLM (BF16)\n",
    "# =====================================================\n",
    "\n",
    "llm_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "llm_hidden = llm.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3️⃣ Projection Layer (Trainable)\n",
    "# =====================================================\n",
    "\n",
    "projector = nn.Linear(768, llm_hidden, bias=False).to(device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------\n",
    "# Single Image + Caption\n",
    "# ----------------------------------\n",
    "\n",
    "image = Image.open(\"dogs-playing-in-grassy-field.jpg\").convert(\"RGB\")\n",
    "caption = \"Two dogs playing.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode image once (frozen)\n",
    "inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    vision_outputs = vision_model(**inputs)\n",
    "    patch_tokens_raw = vision_outputs.last_hidden_state.detach()  # (1,196,768)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 5️⃣ Tokenize Caption ONLY\n",
    "# =====================================================\n",
    "\n",
    "tokens = tokenizer(\n",
    "    caption,\n",
    "    return_tensors=\"pt\"\n",
    ").to(device)\n",
    "\n",
    "input_ids = tokens.input_ids\n",
    "attention_mask = tokens.attention_mask\n",
    "\n",
    "# We want to predict ALL tokens\n",
    "labels = input_ids.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=1024, bias=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 6️⃣ Optimizer\n",
    "# =====================================================\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(projector.parameters()) + list(llm.parameters()),\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "llm.train()\n",
    "projector.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Loss: 14.136856079101562\n",
      "Step 50 | Loss: 0.00236580241471529\n",
      "Step 100 | Loss: 0.0013496836181730032\n",
      "Step 150 | Loss: 0.0009765183785930276\n",
      "Step 200 | Loss: 0.0007501288782805204\n",
      "Step 250 | Loss: 0.0005781014915555716\n",
      "Step 300 | Loss: 0.0004980136873200536\n",
      "Step 350 | Loss: 0.00045549171045422554\n",
      "Step 400 | Loss: 0.0004082410014234483\n",
      "Step 450 | Loss: 0.00036625508801080287\n",
      "Outputs Logits Shape:  torch.Size([1, 200, 151936])\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 7️⃣ Training Loop\n",
    "# =====================================================\n",
    "\n",
    "for step in range(500):\n",
    "\n",
    "    # Project vision tokens inside loop\n",
    "    patch_tokens = projector(patch_tokens_raw)\n",
    "\n",
    "    # Text embeddings\n",
    "    text_embeds = llm.get_input_embeddings()(input_ids)\n",
    "\n",
    "    # Concatenate vision + text\n",
    "    inputs_embeds = torch.cat([patch_tokens, text_embeds], dim=1)\n",
    "\n",
    "    # Attention mask\n",
    "    visual_attention = torch.ones(\n",
    "        (1, patch_tokens.size(1)),\n",
    "        device=device,\n",
    "        dtype=attention_mask.dtype\n",
    "    )\n",
    "\n",
    "    full_attention = torch.cat(\n",
    "        [visual_attention, attention_mask],\n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "    # Mask visual token positions in loss\n",
    "    visual_label_pad = torch.full(\n",
    "        (1, patch_tokens.size(1)),\n",
    "        -100,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    full_labels = torch.cat(\n",
    "        [visual_label_pad, labels],\n",
    "        dim=1\n",
    "    )\n",
    "\n",
    "    outputs = llm(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=full_attention,\n",
    "        labels=full_labels\n",
    "    )\n",
    "\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 50 == 0:\n",
    "        print(f\"Step {step} | Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Outputs Logits Shape: \", outputs.logits.shape) # (batch, seq_len, vocab_size) -> (1, 200 (196 visual tokens + 4 text), vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== WITH VISION ===\n",
      "Two dogs playing...... playing\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 8️⃣ Inference Test (With Vision)\n",
    "# =====================================================\n",
    "\n",
    "llm.eval()\n",
    "projector.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    patch_tokens = projector(patch_tokens_raw)\n",
    "\n",
    "    # No text prompt at all\n",
    "    inputs_embeds = patch_tokens\n",
    "\n",
    "    visual_attention = torch.ones(\n",
    "        (1, patch_tokens.size(1)),\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    generated = llm.generate(\n",
    "        inputs_embeds=inputs_embeds,\n",
    "        attention_mask=visual_attention,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "print(\"\\n=== WITH VISION ===\")\n",
    "print(tokenizer.decode(generated[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ZERO VISION ===\n",
      "!lll. 2. 3.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 9️⃣ Ablation Test (Zero Vision)\n",
    "# =====================================================\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    zero_visual = torch.zeros_like(patch_tokens)\n",
    "\n",
    "    generated_zero = llm.generate(\n",
    "        inputs_embeds=zero_visual,\n",
    "        attention_mask=visual_attention,\n",
    "        max_new_tokens=10,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "print(\"\\n=== ZERO VISION ===\")\n",
    "print(tokenizer.decode(generated_zero[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "Image: dogs-playing-in-grassy-field.jpg\n",
      "WITH VISION : Two dogs playing...... playing\n",
      "ZERO VISION : !lll. 2. 3.\n",
      "\n",
      "======================================\n",
      "Image: dog_and_girl.jpeg\n",
      "WITH VISION : Two dogs playing.......\n",
      "ZERO VISION : !lll. 2. 3.\n",
      "\n",
      "======================================\n",
      "Image: sample.png\n",
      "WITH VISION : Two dogs playing.. playing....\n",
      "ZERO VISION : !lll. 2. 3.\n",
      "\n",
      "======================================\n",
      "Image: dog.png\n",
      "WITH VISION : Two dogs playing.......\n",
      "ZERO VISION : !lll. 2. 3.\n",
      "\n",
      "======================================\n",
      "Image: gd-dog.jpg\n",
      "WITH VISION : Two dogs playing.......\n",
      "ZERO VISION : !lll. 2. 3.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Inference Test (With Vision) - Different Image\n",
    "# =====================================================\n",
    "\n",
    "llm.eval()\n",
    "projector.eval()\n",
    "\n",
    "# =====================================================\n",
    "# Test Images Folder\n",
    "# =====================================================\n",
    "\n",
    "image_folder = \"images\"  # put multiple images here\n",
    "image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder)]\n",
    "\n",
    "# =====================================================\n",
    "# Evaluation Loop\n",
    "# =====================================================\n",
    "\n",
    "for img_path in image_paths:\n",
    "\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"Image:\", img_path.split(\"/\")[-1])\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vision_outputs = vision_model(**inputs)\n",
    "        patch_tokens_raw = vision_outputs.last_hidden_state\n",
    "\n",
    "        patch_tokens = projector(patch_tokens_raw)\n",
    "\n",
    "        visual_attention = torch.ones(\n",
    "            (1, patch_tokens.size(1)),\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "    # -------------------------\n",
    "    # WITH VISION\n",
    "    # -------------------------\n",
    "    with torch.no_grad():\n",
    "\n",
    "        generated = llm.generate(\n",
    "            inputs_embeds=patch_tokens,\n",
    "            attention_mask=visual_attention,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    output_with_vision = tokenizer.decode(\n",
    "        generated[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # ZERO VISION\n",
    "    # -------------------------\n",
    "    with torch.no_grad():\n",
    "\n",
    "        zero_visual = torch.zeros_like(patch_tokens)\n",
    "\n",
    "        generated_zero = llm.generate(\n",
    "            inputs_embeds=zero_visual,\n",
    "            attention_mask=visual_attention,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    output_zero = tokenizer.decode(\n",
    "        generated_zero[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Print Results\n",
    "    # -------------------------\n",
    "\n",
    "    print(\"WITH VISION :\", output_with_vision)\n",
    "    print(\"ZERO VISION :\", output_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure grounding metrics\n",
    "# we compute\n",
    "\n",
    "# Δ = || logits_with_vision − logits_without_vision ||\n",
    "\n",
    "def measure_vision_influence(patch_tokens):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Random baseline instead of zero\n",
    "        random_visual = torch.randn_like(patch_tokens) * 0.01\n",
    "\n",
    "        logits_v = llm(inputs_embeds=patch_tokens).logits[:, -1, :]\n",
    "        logits_r = llm(inputs_embeds=random_visual).logits[:, -1, :]\n",
    "\n",
    "        diff = torch.norm(logits_v - logits_r).item()\n",
    "        base = torch.norm(logits_r).item()\n",
    "\n",
    "        relative = diff / (base + 1e-8)\n",
    "\n",
    "    print(f\"Vision influence (L2 diff): {diff:.4f}\")\n",
    "    print(f\"Baseline norm: {base:.4f}\")\n",
    "    print(f\"Relative influence: {relative:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================\n",
      "Image: dogs-playing-in-grassy-field.jpg\n",
      "Vision influence (L2 diff): 2528.0000\n",
      "Baseline norm: 868.0000\n",
      "Relative influence: 2.9124\n",
      "WITH VISION : Two dogs playing...... playing\n",
      "ZERO VISION : !lll. 2. 3.\n",
      "\n",
      "======================================\n",
      "Image: dog_and_girl.jpeg\n",
      "Vision influence (L2 diff): 2240.0000\n",
      "Baseline norm: 732.0000\n",
      "Relative influence: 3.0601\n",
      "WITH VISION : Two dogs playing.......\n",
      "ZERO VISION : !lll. 2. 3.\n",
      "\n",
      "======================================\n",
      "Image: sample.png\n",
      "Vision influence (L2 diff): 2160.0000\n",
      "Baseline norm: 696.0000\n",
      "Relative influence: 3.1034\n",
      "WITH VISION : Two dogs playing.. playing....\n",
      "ZERO VISION : !lll. 2. 3.\n",
      "\n",
      "======================================\n",
      "Image: dog.png\n",
      "Vision influence (L2 diff): 2144.0000\n",
      "Baseline norm: 884.0000\n",
      "Relative influence: 2.4253\n",
      "WITH VISION : Two dogs playing.......\n",
      "ZERO VISION : !lll. 2. 3.\n",
      "\n",
      "======================================\n",
      "Image: gd-dog.jpg\n",
      "Vision influence (L2 diff): 2512.0000\n",
      "Baseline norm: 784.0000\n",
      "Relative influence: 3.2041\n",
      "WITH VISION : Two dogs playing.......\n",
      "ZERO VISION : !lll. 2. 3.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Inference Test (With Vision) - Different Image\n",
    "# =====================================================\n",
    "\n",
    "llm.eval()\n",
    "projector.eval()\n",
    "\n",
    "# =====================================================\n",
    "# Test Images Folder\n",
    "# =====================================================\n",
    "\n",
    "image_folder = \"images\"  # put multiple images here\n",
    "image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder)]\n",
    "\n",
    "# =====================================================\n",
    "# Evaluation Loop\n",
    "# =====================================================\n",
    "\n",
    "for img_path in image_paths:\n",
    "\n",
    "    print(\"\\n======================================\")\n",
    "    print(\"Image:\", img_path.split(\"/\")[-1])\n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        vision_outputs = vision_model(**inputs)\n",
    "        patch_tokens_raw = vision_outputs.last_hidden_state\n",
    "\n",
    "        patch_tokens = projector(patch_tokens_raw)\n",
    "\n",
    "        visual_attention = torch.ones(\n",
    "            (1, patch_tokens.size(1)),\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        measure_vision_influence(patch_tokens)\n",
    "\n",
    "    # -------------------------\n",
    "    # WITH VISION\n",
    "    # -------------------------\n",
    "    with torch.no_grad():\n",
    "\n",
    "        generated = llm.generate(\n",
    "            inputs_embeds=patch_tokens,\n",
    "            attention_mask=visual_attention,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    output_with_vision = tokenizer.decode(\n",
    "        generated[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # ZERO VISION\n",
    "    # -------------------------\n",
    "    with torch.no_grad():\n",
    "\n",
    "        zero_visual = torch.zeros_like(patch_tokens)\n",
    "\n",
    "        generated_zero = llm.generate(\n",
    "            inputs_embeds=zero_visual,\n",
    "            attention_mask=visual_attention,\n",
    "            max_new_tokens=10,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    output_zero = tokenizer.decode(\n",
    "        generated_zero[0],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # Print Results\n",
    "    # -------------------------\n",
    "\n",
    "    print(\"WITH VISION :\", output_with_vision)\n",
    "    print(\"ZERO VISION :\", output_zero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
