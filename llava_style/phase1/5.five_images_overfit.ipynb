{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1️⃣ Load Vision Encoder (Frozen)\n",
    "# =====================================================\n",
    "\n",
    "vision_name = \"google/siglip-base-patch16-224\"\n",
    "processor = AutoProcessor.from_pretrained(vision_name, use_fast=True)\n",
    "\n",
    "vision_model = AutoModel.from_pretrained(\n",
    "    vision_name\n",
    ").vision_model.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "vision_model.eval()\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 2️⃣ Load LLM\n",
    "# =====================================================\n",
    "\n",
    "llm_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "llm_hidden = llm.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3️⃣ Projection Layer\n",
    "# =====================================================\n",
    "\n",
    "projector = nn.Linear(768, llm_hidden, bias=False).to(device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 4️⃣ Load 5 Images + Captions\n",
    "# =====================================================\n",
    "\n",
    "data = [\n",
    "    (\"images/airplane.png\",\n",
    "     \"A large passenger airplane flying through the air.\"),\n",
    "\n",
    "    (\"images/motorcycle.png\",\n",
    "     \"Riding a motorcycle down a street.\"),\n",
    "\n",
    "    (\"images/gd-dog.jpg\",\n",
    "     \"A dog standing in a grassy field.\"),\n",
    "\n",
    "    (\"images/kitchen.png\",\n",
    "     \"A kitchen stove, sink, and counter with stuff on it.\"),\n",
    "\n",
    "    (\"images/person-umbrella.png\",\n",
    "     \"A person walking in the rain while holding an umbrella.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = vision_model(**inputs)\n",
    "        return outputs.last_hidden_state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute visual tokens\n",
    "dataset = []\n",
    "for img_path, caption in data:\n",
    "    patch = encode_image(img_path)\n",
    "    tokens = tokenizer(caption, return_tensors=\"pt\").to(device)\n",
    "    dataset.append((patch, tokens, caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 5️⃣ Optimizer\n",
    "# =====================================================\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(projector.parameters()) + list(llm.parameters()),\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Avg Loss: 8.819773\n",
      "Step 200 | Avg Loss: 0.000247\n",
      "Step 400 | Avg Loss: 0.000174\n",
      "Step 600 | Avg Loss: 0.000145\n",
      "Step 800 | Avg Loss: 0.000135\n",
      "Step 1000 | Avg Loss: 0.000126\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 6️⃣ Training Loop\n",
    "# =====================================================\n",
    "\n",
    "llm.train()\n",
    "projector.train()\n",
    "\n",
    "for step in range(1200):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for patch_raw, tokens, _ in dataset:\n",
    "\n",
    "        patch_tokens = projector(patch_raw)\n",
    "\n",
    "        text_embeds = llm.get_input_embeddings()(tokens.input_ids)\n",
    "\n",
    "        inputs_embeds = torch.cat([patch_tokens, text_embeds], dim=1)\n",
    "\n",
    "        visual_attention = torch.ones(\n",
    "            (1, patch_tokens.size(1)),\n",
    "            device=device,\n",
    "            dtype=tokens.attention_mask.dtype\n",
    "        )\n",
    "\n",
    "        full_attention = torch.cat(\n",
    "            [visual_attention, tokens.attention_mask],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        visual_label_pad = torch.full(\n",
    "            (1, patch_tokens.size(1)),\n",
    "            -100,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        full_labels = torch.cat(\n",
    "            [visual_label_pad, tokens.input_ids],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        outputs = llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=full_attention,\n",
    "            labels=full_labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step} | Avg Loss: {total_loss/len(dataset):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST RESULTS ===\n",
      "Target  : A large passenger airplane flying through the air.\n",
      "Output  : A large passenger airplane flying through the air. air. air. air.\n",
      "--------\n",
      "Target  : Riding a motorcycle down a street.\n",
      "Output  : Riding a motorcycle down a street.iding a street.iding a motorcycle\n",
      "--------\n",
      "Target  : A dog standing in a grassy field.\n",
      "Output  : A dog standing in a grassy field. the the in theyy\n",
      "--------\n",
      "Target  : A kitchen stove, sink, and counter with stuff on it.\n",
      "Output  : A kitchen stove, sink, and counter with stuff on it. and down\n",
      "--------\n",
      "Target  : A person walking in the rain while holding an umbrella.\n",
      "Output  : A person walking in the rain while holding an umbrella. the the the the\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 7️⃣ Evaluation\n",
    "# =====================================================\n",
    "\n",
    "llm.eval()\n",
    "projector.eval()\n",
    "\n",
    "print(\"\\n=== TEST RESULTS ===\")\n",
    "\n",
    "for patch_raw, _, caption in dataset:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        patch_tokens = projector(patch_raw)\n",
    "\n",
    "        generated = llm.generate(\n",
    "            inputs_embeds=patch_tokens,\n",
    "            attention_mask=torch.ones(\n",
    "                (1, patch_tokens.size(1)),\n",
    "                device=device\n",
    "            ),\n",
    "            max_new_tokens=15,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Target  :\", caption)\n",
    "    print(\"Output  :\", output)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ZERO VISION TEST ===\n",
      "Original Caption → A large passenger airplane flying through the air.\n",
      "Zero vision → ! * 1.5. The air. The air. The air\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# ZERO VISION TEST\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n=== ZERO VISION TEST ===\")\n",
    "\n",
    "zero_tokens = torch.zeros_like(projector(dataset[0][0]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_zero = llm.generate(\n",
    "        inputs_embeds=zero_tokens,\n",
    "        attention_mask=torch.ones(\n",
    "            (1, zero_tokens.size(1)),\n",
    "            device=device\n",
    "        ),\n",
    "        max_new_tokens=15,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "print(\"Original Caption →\", dataset[0][2])\n",
    "print(\"Zero vision →\", tokenizer.decode(generated_zero[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
