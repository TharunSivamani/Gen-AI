{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¬ These Three Correspond To Three Research Questions\n",
    "# Method\tTests\n",
    "# First-K\tpositional bias\n",
    "# Uniform\tspatial sparsity\n",
    "# Mean Pool\tspatial resolution importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "import itertools\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# SEARCH SPACE\n",
    "# =====================================================\n",
    "\n",
    "TOKEN_COUNTS = [196, 64, 32, 16, 8]\n",
    "REDUCTION_METHODS = [\"uniform\", \"meanpool\"]\n",
    "PROJECTOR_TYPES = [\"linear\", \"mlp\"]\n",
    "LEARNING_RATES = [1e-4, 5e-5]\n",
    "\n",
    "TRAIN_STEPS = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 4ï¸âƒ£ Load 5 Images + Captions\n",
    "# =====================================================\n",
    "\n",
    "data = [\n",
    "    (\"images/airplane.png\",\n",
    "     \"A large passenger airplane flying through the air.\"),\n",
    "\n",
    "    (\"images/motorcycle.png\",\n",
    "     \"Riding a motorcycle down a street.\"),\n",
    "\n",
    "    (\"images/gd-dog.jpg\",\n",
    "     \"A dog standing in a grassy field.\"),\n",
    "\n",
    "    (\"images/kitchen.png\",\n",
    "     \"A kitchen stove, sink, and counter with stuff on it.\"),\n",
    "\n",
    "    (\"images/person-umbrella.png\",\n",
    "     \"A person walking in the rain while holding an umbrella.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Token Reduction\n",
    "# =====================================================\n",
    "\n",
    "def reduce_tokens(patch, method, K):\n",
    "\n",
    "    if K >= patch.size(1):\n",
    "        return patch\n",
    "\n",
    "    if method == \"uniform\":\n",
    "        indices = torch.linspace(\n",
    "            0, patch.size(1)-1, K\n",
    "        ).long().to(patch.device)\n",
    "        return patch[:, indices, :]\n",
    "\n",
    "    elif method == \"meanpool\":\n",
    "        N = patch.size(1)\n",
    "        chunk = N // K\n",
    "        pooled = []\n",
    "        for i in range(K):\n",
    "            pooled.append(\n",
    "                patch[:, i*chunk:(i+1)*chunk, :].mean(dim=1, keepdim=True)\n",
    "            )\n",
    "        return torch.cat(pooled, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Vision Influence Metric\n",
    "# =====================================================\n",
    "\n",
    "def vision_influence(llm, patch_tokens):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        random_visual = torch.randn_like(patch_tokens) * 0.01\n",
    "\n",
    "        logits_v = llm(inputs_embeds=patch_tokens).logits[:, -1, :]\n",
    "        logits_r = llm(inputs_embeds=random_visual).logits[:, -1, :]\n",
    "\n",
    "        diff = torch.norm(logits_v - logits_r).item()\n",
    "        base = torch.norm(logits_r).item()\n",
    "\n",
    "    return diff / (base + 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Run Single Experiment\n",
    "# =====================================================\n",
    "\n",
    "def run_experiment(token_count, method, projector_type, lr):\n",
    "\n",
    "    print(f\"\\nRunning: tokens={token_count}, method={method}, \"\n",
    "          f\"proj={projector_type}, lr={lr}\")\n",
    "\n",
    "    # Load models fresh each run\n",
    "    vision_name = \"google/siglip-base-patch16-224\"\n",
    "    processor = AutoProcessor.from_pretrained(vision_name, use_fast=True)\n",
    "\n",
    "    vision_model = AutoModel.from_pretrained(\n",
    "        vision_name\n",
    "    ).vision_model.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "    vision_model.eval()\n",
    "    for p in vision_model.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    llm_name = \"Qwen/Qwen3-0.6B\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    llm = AutoModelForCausalLM.from_pretrained(\n",
    "        llm_name,\n",
    "        dtype=torch.bfloat16\n",
    "    ).to(device)\n",
    "\n",
    "    hidden = llm.config.hidden_size\n",
    "\n",
    "    # Projector\n",
    "    if projector_type == \"linear\":\n",
    "        projector = nn.Linear(768, hidden, bias=False)\n",
    "\n",
    "    else:\n",
    "        projector = nn.Sequential(\n",
    "            nn.Linear(768, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden, hidden)\n",
    "        )\n",
    "\n",
    "    projector = projector.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        list(projector.parameters()) + list(llm.parameters()),\n",
    "        lr=lr\n",
    "    )\n",
    "\n",
    "    # Precompute patches\n",
    "    dataset = []\n",
    "\n",
    "    for img_path, caption in data:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = vision_model(**inputs)\n",
    "            patch = outputs.last_hidden_state.detach()\n",
    "\n",
    "        patch = reduce_tokens(patch, method, token_count)\n",
    "\n",
    "        tokens = tokenizer(caption, return_tensors=\"pt\").to(device)\n",
    "        dataset.append((patch, tokens, caption))\n",
    "\n",
    "    # Training\n",
    "    llm.train()\n",
    "    projector.train()\n",
    "\n",
    "    for step in range(0, TRAIN_STEPS+1):\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        for patch_raw, tokens, _ in dataset:\n",
    "\n",
    "            patch_tokens = projector(patch_raw)\n",
    "\n",
    "            text_embeds = llm.get_input_embeddings()(tokens.input_ids)\n",
    "\n",
    "            inputs_embeds = torch.cat([patch_tokens, text_embeds], dim=1)\n",
    "\n",
    "            visual_attention = torch.ones(\n",
    "                (1, patch_tokens.size(1)),\n",
    "                device=device,\n",
    "                dtype=tokens.attention_mask.dtype\n",
    "            )\n",
    "\n",
    "            full_attention = torch.cat(\n",
    "                [visual_attention, tokens.attention_mask],\n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "            visual_label_pad = torch.full(\n",
    "                (1, patch_tokens.size(1)),\n",
    "                -100,\n",
    "                device=device\n",
    "            )\n",
    "\n",
    "            full_labels = torch.cat(\n",
    "                [visual_label_pad, tokens.input_ids],\n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "            outputs = llm(\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                attention_mask=full_attention,\n",
    "                labels=full_labels\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    final_loss = total_loss / len(dataset)\n",
    "\n",
    "    # Evaluation accuracy\n",
    "    llm.eval()\n",
    "    projector.eval()\n",
    "\n",
    "    correct = 0\n",
    "\n",
    "    for patch_raw, _, caption in dataset:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            patch_tokens = projector(patch_raw)\n",
    "\n",
    "            generated = llm.generate(\n",
    "                inputs_embeds=patch_tokens,\n",
    "                attention_mask=torch.ones(\n",
    "                    (1, patch_tokens.size(1)),\n",
    "                    device=device\n",
    "                ),\n",
    "                max_new_tokens=15,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        output = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "        if caption in output:\n",
    "            correct += 1\n",
    "\n",
    "    accuracy = correct / len(dataset)\n",
    "\n",
    "    influence = vision_influence(llm, projector(dataset[0][0]))\n",
    "\n",
    "    # Cleanup\n",
    "    del llm\n",
    "    del projector\n",
    "    del vision_model\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    return {\n",
    "        \"tokens\": token_count,\n",
    "        \"method\": method,\n",
    "        \"projector\": projector_type,\n",
    "        \"lr\": lr,\n",
    "        \"loss\": final_loss,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"influence\": influence\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: tokens=196, method=uniform, proj=linear, lr=0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running: tokens=196, method=uniform, proj=linear, lr=5e-05\n",
      "\n",
      "Running: tokens=196, method=uniform, proj=mlp, lr=0.0001\n",
      "\n",
      "Running: tokens=196, method=uniform, proj=mlp, lr=5e-05\n",
      "\n",
      "Running: tokens=196, method=meanpool, proj=linear, lr=0.0001\n",
      "\n",
      "Running: tokens=196, method=meanpool, proj=linear, lr=5e-05\n",
      "\n",
      "Running: tokens=196, method=meanpool, proj=mlp, lr=0.0001\n",
      "\n",
      "Running: tokens=196, method=meanpool, proj=mlp, lr=5e-05\n",
      "\n",
      "Running: tokens=64, method=uniform, proj=linear, lr=0.0001\n",
      "\n",
      "Running: tokens=64, method=uniform, proj=linear, lr=5e-05\n",
      "\n",
      "Running: tokens=64, method=uniform, proj=mlp, lr=0.0001\n",
      "\n",
      "Running: tokens=64, method=uniform, proj=mlp, lr=5e-05\n",
      "\n",
      "Running: tokens=64, method=meanpool, proj=linear, lr=0.0001\n",
      "\n",
      "Running: tokens=64, method=meanpool, proj=linear, lr=5e-05\n",
      "\n",
      "Running: tokens=64, method=meanpool, proj=mlp, lr=0.0001\n",
      "\n",
      "Running: tokens=64, method=meanpool, proj=mlp, lr=5e-05\n",
      "\n",
      "Running: tokens=32, method=uniform, proj=linear, lr=0.0001\n",
      "\n",
      "Running: tokens=32, method=uniform, proj=linear, lr=5e-05\n",
      "\n",
      "Running: tokens=32, method=uniform, proj=mlp, lr=0.0001\n",
      "\n",
      "Running: tokens=32, method=uniform, proj=mlp, lr=5e-05\n",
      "\n",
      "Running: tokens=32, method=meanpool, proj=linear, lr=0.0001\n",
      "\n",
      "Running: tokens=32, method=meanpool, proj=linear, lr=5e-05\n",
      "\n",
      "Running: tokens=32, method=meanpool, proj=mlp, lr=0.0001\n",
      "\n",
      "Running: tokens=32, method=meanpool, proj=mlp, lr=5e-05\n",
      "\n",
      "Running: tokens=16, method=uniform, proj=linear, lr=0.0001\n",
      "\n",
      "Running: tokens=16, method=uniform, proj=linear, lr=5e-05\n",
      "\n",
      "Running: tokens=16, method=uniform, proj=mlp, lr=0.0001\n",
      "\n",
      "Running: tokens=16, method=uniform, proj=mlp, lr=5e-05\n",
      "\n",
      "Running: tokens=16, method=meanpool, proj=linear, lr=0.0001\n",
      "\n",
      "Running: tokens=16, method=meanpool, proj=linear, lr=5e-05\n",
      "\n",
      "Running: tokens=16, method=meanpool, proj=mlp, lr=0.0001\n",
      "\n",
      "Running: tokens=16, method=meanpool, proj=mlp, lr=5e-05\n",
      "\n",
      "Running: tokens=8, method=uniform, proj=linear, lr=0.0001\n",
      "\n",
      "Running: tokens=8, method=uniform, proj=linear, lr=5e-05\n",
      "\n",
      "Running: tokens=8, method=uniform, proj=mlp, lr=0.0001\n",
      "\n",
      "Running: tokens=8, method=uniform, proj=mlp, lr=5e-05\n",
      "\n",
      "Running: tokens=8, method=meanpool, proj=linear, lr=0.0001\n",
      "\n",
      "Running: tokens=8, method=meanpool, proj=linear, lr=5e-05\n",
      "\n",
      "Running: tokens=8, method=meanpool, proj=mlp, lr=0.0001\n",
      "\n",
      "Running: tokens=8, method=meanpool, proj=mlp, lr=5e-05\n",
      "\n",
      "\n",
      "===== FINAL RESULTS =====\n",
      "{'tokens': 16, 'method': 'uniform', 'projector': 'mlp', 'lr': 0.0001, 'loss': 4.81784030853305e-05, 'accuracy': 1.0, 'influence': 0.6369426751567001}\n",
      "{'tokens': 8, 'method': 'uniform', 'projector': 'linear', 'lr': 0.0001, 'loss': 6.429997374652885e-05, 'accuracy': 1.0, 'influence': 1.8799999999812}\n",
      "{'tokens': 64, 'method': 'uniform', 'projector': 'linear', 'lr': 0.0001, 'loss': 7.706647302256897e-05, 'accuracy': 1.0, 'influence': 1.519230769218596}\n",
      "{'tokens': 32, 'method': 'meanpool', 'projector': 'mlp', 'lr': 0.0001, 'loss': 8.83294313098304e-05, 'accuracy': 1.0, 'influence': 1.1946308724731995}\n",
      "{'tokens': 64, 'method': 'meanpool', 'projector': 'mlp', 'lr': 0.0001, 'loss': 9.133219973591621e-05, 'accuracy': 1.0, 'influence': 1.725806451595506}\n",
      "{'tokens': 8, 'method': 'uniform', 'projector': 'mlp', 'lr': 0.0001, 'loss': 9.227033151546494e-05, 'accuracy': 1.0, 'influence': 0.41551724137751933}\n",
      "{'tokens': 8, 'method': 'meanpool', 'projector': 'linear', 'lr': 0.0001, 'loss': 9.266855995520018e-05, 'accuracy': 1.0, 'influence': 0.8333333333280725}\n",
      "{'tokens': 16, 'method': 'meanpool', 'projector': 'linear', 'lr': 5e-05, 'loss': 9.925293634296394e-05, 'accuracy': 1.0, 'influence': 3.3782383419251523}\n",
      "{'tokens': 16, 'method': 'uniform', 'projector': 'mlp', 'lr': 5e-05, 'loss': 0.00011652647808659821, 'accuracy': 1.0, 'influence': 1.1772151898641043}\n",
      "{'tokens': 32, 'method': 'uniform', 'projector': 'linear', 'lr': 0.0001, 'loss': 0.00012069992590113543, 'accuracy': 1.0, 'influence': 1.744360902239245}\n",
      "{'tokens': 32, 'method': 'meanpool', 'projector': 'linear', 'lr': 0.0001, 'loss': 0.00012141959014115855, 'accuracy': 1.0, 'influence': 0.9351351351288167}\n",
      "{'tokens': 196, 'method': 'meanpool', 'projector': 'linear', 'lr': 0.0001, 'loss': 0.0001231689428095706, 'accuracy': 1.0, 'influence': 1.5874999999875978}\n",
      "{'tokens': 196, 'method': 'meanpool', 'projector': 'mlp', 'lr': 0.0001, 'loss': 0.0001361605041893199, 'accuracy': 1.0, 'influence': 1.02985074625905}\n",
      "{'tokens': 8, 'method': 'meanpool', 'projector': 'linear', 'lr': 5e-05, 'loss': 0.0001436718986951746, 'accuracy': 1.0, 'influence': 2.082568807315567}\n",
      "{'tokens': 64, 'method': 'uniform', 'projector': 'mlp', 'lr': 0.0001, 'loss': 0.00014379161002580075, 'accuracy': 1.0, 'influence': 1.3536585365750484}\n",
      "{'tokens': 196, 'method': 'uniform', 'projector': 'mlp', 'lr': 0.0001, 'loss': 0.00014813360467087479, 'accuracy': 1.0, 'influence': 0.7394957983154439}\n",
      "{'tokens': 32, 'method': 'uniform', 'projector': 'mlp', 'lr': 5e-05, 'loss': 0.00014903056871844455, 'accuracy': 1.0, 'influence': 2.0365296803420487}\n",
      "{'tokens': 64, 'method': 'meanpool', 'projector': 'linear', 'lr': 0.0001, 'loss': 0.00014917042717570438, 'accuracy': 1.0, 'influence': 1.4155251141471716}\n",
      "{'tokens': 16, 'method': 'meanpool', 'projector': 'linear', 'lr': 0.0001, 'loss': 0.00016137274069478736, 'accuracy': 1.0, 'influence': 1.5967741935322906}\n",
      "{'tokens': 64, 'method': 'uniform', 'projector': 'mlp', 'lr': 5e-05, 'loss': 0.00016590058512520046, 'accuracy': 1.0, 'influence': 1.4666666666522876}\n",
      "{'tokens': 64, 'method': 'meanpool', 'projector': 'linear', 'lr': 5e-05, 'loss': 0.0001739227052894421, 'accuracy': 1.0, 'influence': 1.3602150537542996}\n",
      "{'tokens': 16, 'method': 'uniform', 'projector': 'linear', 'lr': 5e-05, 'loss': 0.00017519793909741565, 'accuracy': 1.0, 'influence': 1.1666666666579861}\n",
      "{'tokens': 32, 'method': 'uniform', 'projector': 'mlp', 'lr': 0.0001, 'loss': 0.0001804265222745016, 'accuracy': 1.0, 'influence': 0.8340807174841139}\n",
      "{'tokens': 32, 'method': 'uniform', 'projector': 'linear', 'lr': 5e-05, 'loss': 0.0001845246646553278, 'accuracy': 1.0, 'influence': 2.1169590642965357}\n",
      "{'tokens': 196, 'method': 'uniform', 'projector': 'linear', 'lr': 0.0001, 'loss': 0.00018732729076873512, 'accuracy': 1.0, 'influence': 1.595041322297572}\n",
      "{'tokens': 16, 'method': 'meanpool', 'projector': 'mlp', 'lr': 5e-05, 'loss': 0.00018810101028066128, 'accuracy': 1.0, 'influence': 2.8544600938632105}\n",
      "{'tokens': 8, 'method': 'uniform', 'projector': 'mlp', 'lr': 5e-05, 'loss': 0.00019441460899543016, 'accuracy': 1.0, 'influence': 2.0471698112966137}\n",
      "{'tokens': 32, 'method': 'meanpool', 'projector': 'mlp', 'lr': 5e-05, 'loss': 0.00019993624882772565, 'accuracy': 1.0, 'influence': 1.1545454545388947}\n",
      "{'tokens': 64, 'method': 'uniform', 'projector': 'linear', 'lr': 5e-05, 'loss': 0.00020665333431679755, 'accuracy': 1.0, 'influence': 2.5684210525977837}\n",
      "{'tokens': 196, 'method': 'meanpool', 'projector': 'mlp', 'lr': 5e-05, 'loss': 0.0002121150610037148, 'accuracy': 1.0, 'influence': 0.8765957446761884}\n",
      "{'tokens': 16, 'method': 'uniform', 'projector': 'linear', 'lr': 0.0001, 'loss': 0.00022339132556226104, 'accuracy': 1.0, 'influence': 1.916666666644483}\n",
      "{'tokens': 8, 'method': 'meanpool', 'projector': 'mlp', 'lr': 0.0001, 'loss': 0.0002241398993646726, 'accuracy': 1.0, 'influence': 1.1935483870871488}\n",
      "{'tokens': 16, 'method': 'meanpool', 'projector': 'mlp', 'lr': 0.0001, 'loss': 0.0002460214484017342, 'accuracy': 1.0, 'influence': 2.1384615384409766}\n",
      "{'tokens': 64, 'method': 'meanpool', 'projector': 'mlp', 'lr': 5e-05, 'loss': 0.000248963970807381, 'accuracy': 1.0, 'influence': 1.4039735099221526}\n",
      "{'tokens': 8, 'method': 'meanpool', 'projector': 'mlp', 'lr': 5e-05, 'loss': 0.00028666164143942297, 'accuracy': 1.0, 'influence': 0.846153846150148}\n",
      "{'tokens': 8, 'method': 'uniform', 'projector': 'linear', 'lr': 5e-05, 'loss': 0.0002983410726301372, 'accuracy': 1.0, 'influence': 0.7847682119172816}\n",
      "{'tokens': 196, 'method': 'meanpool', 'projector': 'linear', 'lr': 5e-05, 'loss': 0.00030151373357512057, 'accuracy': 1.0, 'influence': 1.6837944663865239}\n",
      "{'tokens': 32, 'method': 'meanpool', 'projector': 'linear', 'lr': 5e-05, 'loss': 0.00031733071664348246, 'accuracy': 1.0, 'influence': 1.420289855059599}\n",
      "{'tokens': 196, 'method': 'uniform', 'projector': 'mlp', 'lr': 5e-05, 'loss': 0.00032562048581894485, 'accuracy': 1.0, 'influence': 2.114035087696118}\n",
      "{'tokens': 196, 'method': 'uniform', 'projector': 'linear', 'lr': 5e-05, 'loss': 0.00037506898806896063, 'accuracy': 1.0, 'influence': 3.0276243093504474}\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# MAIN SEARCH LOOP\n",
    "# =====================================================\n",
    "\n",
    "results = []\n",
    "\n",
    "for config in itertools.product(\n",
    "    TOKEN_COUNTS,\n",
    "    REDUCTION_METHODS,\n",
    "    PROJECTOR_TYPES,\n",
    "    LEARNING_RATES\n",
    "):\n",
    "    results.append(run_experiment(*config))\n",
    "\n",
    "print(\"\\n\\n===== FINAL RESULTS =====\")\n",
    "for r in sorted(results, key=lambda x: (-x[\"accuracy\"], x[\"loss\"])):\n",
    "    print(r)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
