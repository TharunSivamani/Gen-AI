{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start simple.\n",
    "\n",
    "# Train on pairs:\n",
    "\n",
    "# (Image A, Image B) → \"The first image shows an airplane, the second shows a motorcycle.\"\n",
    "\n",
    "# (Image B, Image A) → \"The first image shows a motorcycle, the second shows an airplane.\"\n",
    "\n",
    "# (A) → one sentence\n",
    "\n",
    "# (B) → one sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 1️⃣ Load Vision Encoder (Frozen)\n",
    "# =====================================================\n",
    "\n",
    "vision_name = \"google/siglip-base-patch16-224\"\n",
    "processor = AutoProcessor.from_pretrained(vision_name, use_fast=True)\n",
    "\n",
    "vision_model = AutoModel.from_pretrained(\n",
    "    vision_name\n",
    ").vision_model.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "vision_model.eval()\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 2️⃣ Load LLM\n",
    "# =====================================================\n",
    "\n",
    "llm_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "llm_hidden = llm.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3️⃣ Projection Layer\n",
    "# =====================================================\n",
    "\n",
    "projector = nn.Linear(768, llm_hidden, bias=False).to(device, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Images\n",
    "# =====================================================\n",
    "\n",
    "images = {\n",
    "    \"airplane\": \"images/airplane.png\",\n",
    "    \"motorcycle\": \"images/motorcycle.png\",\n",
    "    \"person\": \"images/person-umbrella.png\",\n",
    "    \"kitchen\": \"images/kitchen.png\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = vision_model(**inputs)\n",
    "        return outputs.last_hidden_state.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute patches\n",
    "patches = {k: encode_image(v) for k, v in images.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Training Pairs\n",
    "# =====================================================\n",
    "\n",
    "train_data = [\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Two-image pairs\n",
    "    # -------------------------------------------------\n",
    "\n",
    "    (\"airplane\", \"motorcycle\",\n",
    "     \"The first image shows an airplane. The second image shows a motorcycle.\"),\n",
    "\n",
    "    (\"motorcycle\", \"airplane\",\n",
    "     \"The first image shows a motorcycle. The second image shows an airplane.\"),\n",
    "\n",
    "    (\"person\", \"kitchen\",\n",
    "     \"The first image shows a person with an umbrella. The second image shows a kitchen.\"),\n",
    "\n",
    "    (\"kitchen\", \"person\",\n",
    "     \"The first image shows a kitchen. The second image shows a person with an umbrella.\"),\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Single-image entries\n",
    "    # -------------------------------------------------\n",
    "\n",
    "    (\"airplane\", None,\n",
    "     \"The image shows an airplane.\"),\n",
    "\n",
    "    (\"motorcycle\", None,\n",
    "     \"The image shows a motorcycle.\"),\n",
    "\n",
    "    (\"person\", None,\n",
    "     \"The image shows a person with an umbrella.\"),\n",
    "\n",
    "    (\"kitchen\", None,\n",
    "     \"The image shows a kitchen.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for img1, img2, caption in train_data:\n",
    "\n",
    "    patch1 = patches[img1]\n",
    "\n",
    "    if img2 is not None:\n",
    "        patch2 = patches[img2]\n",
    "        combined_patch = torch.cat([patch1, patch2], dim=1)\n",
    "    else:\n",
    "        combined_patch = patch1  # single image\n",
    "\n",
    "    combined_patch = combined_patch.to(torch.bfloat16)\n",
    "\n",
    "    tokens = tokenizer(caption, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    dataset.append((combined_patch, tokens, caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# Optimizer\n",
    "# =====================================================\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(projector.parameters()) + list(llm.parameters()),\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Avg Loss: 11.963358\n",
      "Step 200 | Avg Loss: 0.000398\n",
      "Step 400 | Avg Loss: 0.000232\n",
      "Step 600 | Avg Loss: 0.000177\n",
      "Step 800 | Avg Loss: 0.000152\n",
      "Step 1000 | Avg Loss: 0.000138\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Training Loop\n",
    "# =====================================================\n",
    "llm.train()\n",
    "projector.train()\n",
    "\n",
    "for step in range(1001):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for patch_raw, tokens, _ in dataset:\n",
    "\n",
    "        patch_tokens = projector(patch_raw)\n",
    "\n",
    "        text_embeds = llm.get_input_embeddings()(tokens.input_ids)\n",
    "\n",
    "        inputs_embeds = torch.cat([patch_tokens, text_embeds], dim=1)\n",
    "\n",
    "        visual_attention = torch.ones(\n",
    "            (1, patch_tokens.size(1)),\n",
    "            device=device,\n",
    "            dtype=tokens.attention_mask.dtype\n",
    "        )\n",
    "\n",
    "        full_attention = torch.cat(\n",
    "            [visual_attention, tokens.attention_mask],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        visual_label_pad = torch.full(\n",
    "            (1, patch_tokens.size(1)),\n",
    "            -100,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        full_labels = torch.cat(\n",
    "            [visual_label_pad, tokens.input_ids],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        outputs = llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=full_attention,\n",
    "            labels=full_labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step} | Avg Loss: {total_loss/len(dataset):.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST RESULTS ===\n",
      "Target  : The first image shows an airplane. The second image shows a motorcycle.\n",
      "Output  : The first image shows an airplane. The second image shows a motorcycle. The second image shows a motorcycle. The second image shows a motorcycle. The second\n",
      "--------\n",
      "Target  : The first image shows a motorcycle. The second image shows an airplane.\n",
      "Output  : The first image shows a motorcycle. The second image shows an airplane. The second image shows a motorcycle. The second image shows an airplane. The second\n",
      "--------\n",
      "Target  : The first image shows a person with an umbrella. The second image shows a kitchen.\n",
      "Output  : The first image shows a person with an umbrella. The second image shows a kitchen. The second image shows a person with an umbrella. The second image\n",
      "--------\n",
      "Target  : The first image shows a kitchen. The second image shows a person with an umbrella.\n",
      "Output  : The first image shows a kitchen. The second image shows a person with an umbrella. The second image shows a kitchen. The second image shows a person\n",
      "--------\n",
      "Target  : The image shows an airplane.\n",
      "Output  : The image shows an airplane. The image shows an airplane. The image shows a motorcycle. The first image shows an airplane. The image shows a motorcycle\n",
      "--------\n",
      "Target  : The image shows a motorcycle.\n",
      "Output  : The image shows a motorcycle. The second image shows a motorcycle. The second image shows an airplane. The second image shows a motorcycle. The second image\n",
      "--------\n",
      "Target  : The image shows a person with an umbrella.\n",
      "Output  : The image shows a person with an umbrella. The first image shows a person with an umbrella. The second image shows a person with an umbrella. The\n",
      "--------\n",
      "Target  : The image shows a kitchen.\n",
      "Output  : The image shows a kitchen. The second image shows a person with an umbrella. The second image shows a kitchen. The second image shows a kitchen.\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# Evaluation\n",
    "# =====================================================\n",
    "\n",
    "llm.eval()\n",
    "projector.eval()\n",
    "\n",
    "print(\"\\n=== TEST RESULTS ===\")\n",
    "\n",
    "for patch_raw, _, caption in dataset:\n",
    "\n",
    "    with torch.no_grad():\n",
    "        patch_tokens = projector(patch_raw)\n",
    "\n",
    "        generated = llm.generate(\n",
    "            inputs_embeds=patch_tokens,\n",
    "            attention_mask=torch.ones(\n",
    "                (1, patch_tokens.size(1)),\n",
    "                device=device\n",
    "            ),\n",
    "            max_new_tokens=30,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"Target  :\", caption)\n",
    "    print(\"Output  :\", output)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MULTI-IMAGE BINDING TEST ===\n",
      "\n",
      "---------------------------------\n",
      "Entry: (airplane, motorcycle)\n",
      "NORMAL:\n",
      "The first image shows an airplane. The second image shows a motorcycle. The second image shows a motorcycle. The second image shows a motorcycle. The second\n",
      "\n",
      "ZERO SECOND IMAGE:\n",
      ". The first image shows a motorcycle. The second image shows an airplane. The first image shows a motorcycle. The second image shows a person with an\n",
      "\n",
      "ZERO FIRST IMAGE:\n",
      "The image shows a motorcycle. The second image shows a motorcycle. The second image shows an airplane. The second image shows a motorcycle. The second image\n",
      "\n",
      "---------------------------------\n",
      "Entry: (motorcycle, airplane)\n",
      "NORMAL:\n",
      "The first image shows a motorcycle. The second image shows an airplane. The second image shows a motorcycle. The second image shows an airplane. The second\n",
      "\n",
      "ZERO SECOND IMAGE:\n",
      ". The second image shows a motorcycle. The second image shows an airplane. The second image shows a motorcycle. The second image shows a person with an\n",
      "\n",
      "ZERO FIRST IMAGE:\n",
      "The image shows an airplane. The image shows an airplane. The image shows a motorcycle. The first image shows an airplane. The image shows a motorcycle\n",
      "\n",
      "---------------------------------\n",
      "Entry: (person, kitchen)\n",
      "NORMAL:\n",
      "The first image shows a person with an umbrella. The second image shows a kitchen. The second image shows a person with an umbrella. The second image\n",
      "\n",
      "ZERO SECOND IMAGE:\n",
      "The first second second image second image second image shows a person with an umbrella. The second image shows a kitchen. The second image shows a person with\n",
      "\n",
      "ZERO FIRST IMAGE:\n",
      "The image shows a kitchen. The second image shows a person with an umbrella. The second image shows a kitchen. The second image shows a kitchen.\n",
      "\n",
      "---------------------------------\n",
      "Entry: (kitchen, person)\n",
      "NORMAL:\n",
      "The first image shows a kitchen. The second image shows a person with an umbrella. The second image shows a kitchen. The second image shows a person\n",
      "\n",
      "ZERO SECOND IMAGE:\n",
      " second image. The second image shows a kitchen. The second image shows a person with an umbrella. The second image shows a kitchen. The second image\n",
      "\n",
      "ZERO FIRST IMAGE:\n",
      "The image shows a person with an umbrella. The first image shows a person with an umbrella. The first image shows a person with an umbrella. The\n",
      "\n",
      "---------------------------------\n",
      "Entry: (airplane, None)\n",
      "SINGLE IMAGE OUTPUT:\n",
      "The image shows an airplane. The image shows an airplane. The image shows a motorcycle. The first image shows an airplane. The image shows a motorcycle\n",
      "\n",
      "---------------------------------\n",
      "Entry: (motorcycle, None)\n",
      "SINGLE IMAGE OUTPUT:\n",
      "The image shows a motorcycle. The second image shows a motorcycle. The second image shows an airplane. The second image shows a motorcycle. The second image\n",
      "\n",
      "---------------------------------\n",
      "Entry: (person, None)\n",
      "SINGLE IMAGE OUTPUT:\n",
      "The image shows a person with an umbrella. The first image shows a person with an umbrella. The second image shows a person with an umbrella. The\n",
      "\n",
      "---------------------------------\n",
      "Entry: (kitchen, None)\n",
      "SINGLE IMAGE OUTPUT:\n",
      "The image shows a kitchen. The second image shows a person with an umbrella. The second image shows a kitchen. The second image shows a kitchen.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# MULTI-IMAGE BINDING DIAGNOSTICS (UPDATED)\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n=== MULTI-IMAGE BINDING TEST ===\")\n",
    "\n",
    "llm.eval()\n",
    "projector.eval()\n",
    "\n",
    "for img1, img2, caption in train_data:\n",
    "\n",
    "    print(\"\\n---------------------------------\")\n",
    "    print(f\"Entry: ({img1}, {img2})\")\n",
    "\n",
    "    patch1 = patches[img1]\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # CASE 1: Two-image entry\n",
    "    # -------------------------------------------------\n",
    "    if img2 is not None:\n",
    "\n",
    "        patch2 = patches[img2]\n",
    "\n",
    "        combined = torch.cat([patch1, patch2], dim=1).to(torch.bfloat16)\n",
    "\n",
    "        visual_attention = torch.ones(\n",
    "            (1, combined.size(1)),\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            projected = projector(combined)\n",
    "\n",
    "            out_normal = llm.generate(\n",
    "                inputs_embeds=projected,\n",
    "                attention_mask=visual_attention,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        print(\"NORMAL:\")\n",
    "        print(tokenizer.decode(out_normal[0], skip_special_tokens=True))\n",
    "\n",
    "        # -------------------------\n",
    "        # ZERO SECOND IMAGE\n",
    "        # -------------------------\n",
    "        zero_second = torch.cat(\n",
    "            [patch1, torch.zeros_like(patch2)],\n",
    "            dim=1\n",
    "        ).to(torch.bfloat16)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            projected_zero_second = projector(zero_second)\n",
    "\n",
    "            out_zero_second = llm.generate(\n",
    "                inputs_embeds=projected_zero_second,\n",
    "                attention_mask=visual_attention,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        print(\"\\nZERO SECOND IMAGE:\")\n",
    "        print(tokenizer.decode(out_zero_second[0], skip_special_tokens=True))\n",
    "\n",
    "        # -------------------------\n",
    "        # ZERO FIRST IMAGE\n",
    "        # -------------------------\n",
    "        zero_first = torch.cat(\n",
    "            [torch.zeros_like(patch1), patch2],\n",
    "            dim=1\n",
    "        ).to(torch.bfloat16)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            projected_zero_first = projector(zero_first)\n",
    "\n",
    "            out_zero_first = llm.generate(\n",
    "                inputs_embeds=projected_zero_first,\n",
    "                attention_mask=visual_attention,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        print(\"\\nZERO FIRST IMAGE:\")\n",
    "        print(tokenizer.decode(out_zero_first[0], skip_special_tokens=True))\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # CASE 2: Single-image entry\n",
    "    # -------------------------------------------------\n",
    "    else:\n",
    "\n",
    "        combined = patch1.to(torch.bfloat16)\n",
    "\n",
    "        visual_attention = torch.ones(\n",
    "            (1, combined.size(1)),\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            projected = projector(combined)\n",
    "\n",
    "            out_normal = llm.generate(\n",
    "                inputs_embeds=projected,\n",
    "                attention_mask=visual_attention,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False\n",
    "            )\n",
    "\n",
    "        print(\"SINGLE IMAGE OUTPUT:\")\n",
    "        print(tokenizer.decode(out_normal[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
