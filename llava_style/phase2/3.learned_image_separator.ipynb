{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoProcessor, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "from PIL import Image\n",
    "\n",
    "# =====================================================\n",
    "# Setup\n",
    "# =====================================================\n",
    "\n",
    "device = \"cuda\"\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 1️⃣ Load Vision Encoder (Frozen)\n",
    "# =====================================================\n",
    "\n",
    "vision_name = \"google/siglip-base-patch16-224\"\n",
    "processor = AutoProcessor.from_pretrained(vision_name)\n",
    "\n",
    "vision_model = AutoModel.from_pretrained(\n",
    "    vision_name\n",
    ").vision_model.to(device, dtype=torch.bfloat16)\n",
    "\n",
    "vision_model.eval()\n",
    "for p in vision_model.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 2️⃣ Load LLM\n",
    "# =====================================================\n",
    "\n",
    "llm_name = \"Qwen/Qwen3-0.6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    llm_name,\n",
    "    torch_dtype=torch.bfloat16\n",
    ").to(device)\n",
    "\n",
    "hidden = llm.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 3️⃣ Projector + Image Separator\n",
    "# =====================================================\n",
    "\n",
    "projector = nn.Linear(768, hidden, bias=False).to(device, dtype=torch.bfloat16)\n",
    "\n",
    "image_sep = nn.Parameter(\n",
    "    torch.randn(1, 1, hidden, dtype=torch.bfloat16, device=device)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 4️⃣ Load Images\n",
    "# =====================================================\n",
    "\n",
    "images = {\n",
    "    \"airplane\": \"images/airplane.png\",\n",
    "    \"motorcycle\": \"images/motorcycle.png\",\n",
    "    \"person\": \"images/person-umbrella.png\",\n",
    "    \"kitchen\": \"images/kitchen.png\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(path):\n",
    "    image = Image.open(path).convert(\"RGB\")\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = vision_model(**inputs)\n",
    "        return outputs.last_hidden_state.detach()\n",
    "\n",
    "patches = {k: encode_image(v) for k, v in images.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 5️⃣ Training Data\n",
    "# =====================================================\n",
    "\n",
    "train_data = [\n",
    "\n",
    "    # Two-image entries\n",
    "    (\"airplane\", \"motorcycle\",\n",
    "     \"The first image shows an airplane. The second image shows a motorcycle.\"),\n",
    "\n",
    "    (\"motorcycle\", \"airplane\",\n",
    "     \"The first image shows a motorcycle. The second image shows an airplane.\"),\n",
    "\n",
    "    (\"person\", \"kitchen\",\n",
    "     \"The first image shows a person with an umbrella. The second image shows a kitchen.\"),\n",
    "\n",
    "    (\"kitchen\", \"person\",\n",
    "     \"The first image shows a kitchen. The second image shows a person with an umbrella.\"),\n",
    "\n",
    "    # Single-image entries\n",
    "    (\"airplane\", None, \"The image shows an airplane.\"),\n",
    "    (\"motorcycle\", None, \"The image shows a motorcycle.\"),\n",
    "    (\"person\", None, \"The image shows a person with an umbrella.\"),\n",
    "    (\"kitchen\", None, \"The image shows a kitchen.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "for img1, img2, caption in train_data:\n",
    "\n",
    "    if img2 is not None:\n",
    "        dataset.append(((patches[img1], patches[img2]), caption))\n",
    "    else:\n",
    "        dataset.append((patches[img1], caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# 6️⃣ Optimizer\n",
    "# =====================================================\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(projector.parameters()) +\n",
    "    list(llm.parameters()) +\n",
    "    [image_sep],\n",
    "    lr=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 | Avg Loss: 10.557027\n",
      "Step 200 | Avg Loss: 0.000286\n",
      "Step 400 | Avg Loss: 0.000189\n",
      "Step 600 | Avg Loss: 0.000141\n",
      "Step 800 | Avg Loss: 0.000113\n",
      "Step 1000 | Avg Loss: 0.000082\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 7️⃣ Training Loop\n",
    "# =====================================================\n",
    "\n",
    "llm.train()\n",
    "projector.train()\n",
    "\n",
    "for step in range(1001):\n",
    "\n",
    "    total_loss = 0\n",
    "\n",
    "    for visual_data, caption in dataset:\n",
    "\n",
    "        tokens = tokenizer(caption, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Build visual tokens\n",
    "        # -------------------------------------------------\n",
    "\n",
    "        if isinstance(visual_data, tuple):\n",
    "\n",
    "            patch1, patch2 = visual_data\n",
    "\n",
    "            patch1 = patch1.to(torch.bfloat16)\n",
    "            patch2 = patch2.to(torch.bfloat16)\n",
    "\n",
    "            patch1_proj = projector(patch1)\n",
    "            patch2_proj = projector(patch2)\n",
    "\n",
    "            sep = image_sep.expand(1, 1, -1)\n",
    "\n",
    "            visual_tokens = torch.cat(\n",
    "                [patch1_proj, sep, patch2_proj],\n",
    "                dim=1\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            patch1 = visual_data.to(torch.bfloat16)\n",
    "            visual_tokens = projector(patch1)\n",
    "\n",
    "        # -------------------------------------------------\n",
    "        # Text embeddings\n",
    "        # -------------------------------------------------\n",
    "\n",
    "        text_embeds = llm.get_input_embeddings()(tokens.input_ids)\n",
    "\n",
    "        inputs_embeds = torch.cat([visual_tokens, text_embeds], dim=1)\n",
    "\n",
    "        visual_attention = torch.ones(\n",
    "            (1, visual_tokens.size(1)),\n",
    "            device=device,\n",
    "            dtype=tokens.attention_mask.dtype\n",
    "        )\n",
    "\n",
    "        full_attention = torch.cat(\n",
    "            [visual_attention, tokens.attention_mask],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        visual_label_pad = torch.full(\n",
    "            (1, visual_tokens.size(1)),\n",
    "            -100,\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        full_labels = torch.cat(\n",
    "            [visual_label_pad, tokens.input_ids],\n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "        outputs = llm(\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            attention_mask=full_attention,\n",
    "            labels=full_labels\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if step % 200 == 0:\n",
    "        print(f\"Step {step} | Avg Loss: {total_loss/len(dataset):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TEST RESULTS ===\n",
      "Target : The first image shows an airplane. The second image shows a motorcycle.\n",
      "Output : The first image shows an airplane. The second image shows a motorcycle. The second image shows an airplane. The third image shows a motorcycle. The second\n",
      "--------\n",
      "Target : The first image shows a motorcycle. The second image shows an airplane.\n",
      "Output : The first image shows a motorcycle. The second image shows an airplane. The second image shows a motorcycle. The third image shows an airplane. The second\n",
      "--------\n",
      "Target : The first image shows a person with an umbrella. The second image shows a kitchen.\n",
      "Output : The first image shows a person with an umbrella. The second image shows a kitchen. The second image shows a person with an umbrella. The first image\n",
      "--------\n",
      "Target : The first image shows a kitchen. The second image shows a person with an umbrella.\n",
      "Output : The first image shows a kitchen. The second image shows a person with an umbrella. The second image shows a kitchen. The first image shows a person\n",
      "--------\n",
      "Target : The image shows an airplane.\n",
      "Output : The image shows an airplane. The second image shows a motorcycle. The first image shows an airplane. The second image shows a motorcycle. The third image\n",
      "--------\n",
      "Target : The image shows a motorcycle.\n",
      "Output : The image shows a motorcycle. The second image shows an airplane. The third image shows a motorcycle. The second image shows a motorcycle. The third image\n",
      "--------\n",
      "Target : The image shows a person with an umbrella.\n",
      "Output : The image shows a person with an umbrella. The second image shows a kitchen. The first image shows a person with an umbrella. The second image shows\n",
      "--------\n",
      "Target : The image shows a kitchen.\n",
      "Output : The image shows a kitchen. The second image shows a kitchen. The first image shows a kitchen. The second image shows a kitchen. The second image\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 8️⃣ Evaluation\n",
    "# =====================================================\n",
    "\n",
    "llm.eval()\n",
    "projector.eval()\n",
    "\n",
    "print(\"\\n=== TEST RESULTS ===\")\n",
    "\n",
    "for visual_data, caption in dataset:\n",
    "\n",
    "    if isinstance(visual_data, tuple):\n",
    "        patch1, patch2 = visual_data\n",
    "        patch1_proj = projector(patch1.to(torch.bfloat16))\n",
    "        patch2_proj = projector(patch2.to(torch.bfloat16))\n",
    "        sep = image_sep.expand(1, 1, -1)\n",
    "        visual_tokens = torch.cat([patch1_proj, sep, patch2_proj], dim=1)\n",
    "    else:\n",
    "        visual_tokens = projector(visual_data.to(torch.bfloat16))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        generated = llm.generate(\n",
    "            inputs_embeds=visual_tokens,\n",
    "            attention_mask=torch.ones(\n",
    "                (1, visual_tokens.size(1)),\n",
    "                device=device\n",
    "            ),\n",
    "            max_new_tokens=30,\n",
    "            do_sample=False,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    output = tokenizer.decode(generated[0], skip_special_tokens=True).split(tokenizer.eos_token)[0]\n",
    "\n",
    "    print(\"Target :\", caption)\n",
    "    print(\"Output :\", output)\n",
    "    print(\"--------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MULTI-IMAGE BINDING TEST ===\n",
      "\n",
      "---------------------------------\n",
      "Entry: (airplane, motorcycle)\n",
      "NORMAL:\n",
      "The first image shows an airplane. The second image shows a motorcycle. The second image shows an airplane. The third image shows a motorcycle. The second\n",
      "\n",
      "ZERO SECOND IMAGE:\n",
      "The image shows an airplane. The second image shows a motorcycle. The first image shows an airplane. The second image shows a motorcycle. The third image\n",
      "\n",
      "---------------------------------\n",
      "Entry: (motorcycle, airplane)\n",
      "NORMAL:\n",
      "The first image shows a motorcycle. The second image shows an airplane. The second image shows a motorcycle. The third image shows an airplane. The second\n",
      "\n",
      "ZERO SECOND IMAGE:\n",
      "The image shows a motorcycle. The second image shows an airplane. The third image shows a motorcycle. The second image shows a motorcycle. The third image\n",
      "\n",
      "---------------------------------\n",
      "Entry: (person, kitchen)\n",
      "NORMAL:\n",
      "The first image shows a person with an umbrella. The second image shows a kitchen. The second image shows a person with an umbrella. The first image\n",
      "\n",
      "ZERO SECOND IMAGE:\n",
      "The image shows a person with an umbrella. The second image shows a kitchen. The first image shows a person with an umbrella. The second image shows\n",
      "\n",
      "---------------------------------\n",
      "Entry: (kitchen, person)\n",
      "NORMAL:\n",
      "The first image shows a kitchen. The second image shows a person with an umbrella. The second image shows a kitchen. The first image shows a person\n",
      "\n",
      "ZERO SECOND IMAGE:\n",
      "The image shows a kitchen. The second image shows a kitchen. The first image shows a kitchen. The second image shows a kitchen. The second image\n",
      "\n",
      "---------------------------------\n",
      "Entry: (airplane, None)\n",
      "SINGLE IMAGE OUTPUT:\n",
      "The image shows an airplane. The second image shows a motorcycle. The first image shows an airplane. The second image shows a motorcycle. The third image\n",
      "\n",
      "---------------------------------\n",
      "Entry: (motorcycle, None)\n",
      "SINGLE IMAGE OUTPUT:\n",
      "The image shows a motorcycle. The second image shows an airplane. The third image shows a motorcycle. The second image shows a motorcycle. The third image\n",
      "\n",
      "---------------------------------\n",
      "Entry: (person, None)\n",
      "SINGLE IMAGE OUTPUT:\n",
      "The image shows a person with an umbrella. The second image shows a kitchen. The first image shows a person with an umbrella. The second image shows\n",
      "\n",
      "---------------------------------\n",
      "Entry: (kitchen, None)\n",
      "SINGLE IMAGE OUTPUT:\n",
      "The image shows a kitchen. The second image shows a kitchen. The first image shows a kitchen. The second image shows a kitchen. The second image\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# 9️⃣ Multi-Image Binding Diagnostics\n",
    "# =====================================================\n",
    "\n",
    "print(\"\\n=== MULTI-IMAGE BINDING TEST ===\")\n",
    "\n",
    "for img1, img2, caption in train_data:\n",
    "\n",
    "    print(\"\\n---------------------------------\")\n",
    "    print(f\"Entry: ({img1}, {img2})\")\n",
    "\n",
    "    patch1 = patches[img1]\n",
    "\n",
    "    if img2 is not None:\n",
    "\n",
    "        patch2 = patches[img2]\n",
    "\n",
    "        patch1_proj = projector(patch1.to(torch.bfloat16))\n",
    "        patch2_proj = projector(patch2.to(torch.bfloat16))\n",
    "        sep = image_sep.expand(1, 1, -1)\n",
    "\n",
    "        visual_tokens = torch.cat([patch1_proj, sep, patch2_proj], dim=1)\n",
    "\n",
    "        visual_attention = torch.ones(\n",
    "            (1, visual_tokens.size(1)),\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        # NORMAL\n",
    "        with torch.no_grad():\n",
    "            out = llm.generate(\n",
    "                inputs_embeds=visual_tokens,\n",
    "                attention_mask=visual_attention,\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        print(\"NORMAL:\")\n",
    "        print(tokenizer.decode(out[0], skip_special_tokens=True).split(tokenizer.eos_token)[0])\n",
    "\n",
    "        # ZERO SECOND\n",
    "        zero_second_proj = projector(patch1.to(torch.bfloat16))\n",
    "        visual_zero_second = zero_second_proj\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_zero = llm.generate(\n",
    "                inputs_embeds=visual_zero_second,\n",
    "                attention_mask=torch.ones(\n",
    "                    (1, visual_zero_second.size(1)),\n",
    "                    device=device\n",
    "                ),\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        print(\"\\nZERO SECOND IMAGE:\")\n",
    "        print(tokenizer.decode(out_zero[0], skip_special_tokens=True).split(tokenizer.eos_token)[0])\n",
    "\n",
    "    else:\n",
    "        patch1_proj = projector(patch1.to(torch.bfloat16))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = llm.generate(\n",
    "                inputs_embeds=patch1_proj,\n",
    "                attention_mask=torch.ones(\n",
    "                    (1, patch1_proj.size(1)),\n",
    "                    device=device\n",
    "                ),\n",
    "                max_new_tokens=30,\n",
    "                do_sample=False,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        print(\"SINGLE IMAGE OUTPUT:\")\n",
    "        print(tokenizer.decode(out[0], skip_special_tokens=True).split(tokenizer.eos_token)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
